{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full data preprocessing and model training for C-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traditional imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Music and image imports\n",
    "from imageio import imwrite\n",
    "from music21 import converter, instrument, note, chord, converter\n",
    "from PIL import Image, ImageOps\n",
    "import mido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From image to midi file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediary function\n",
    "def column2notes(column, lowerBoundNote = 21):\n",
    "    notes = []\n",
    "    for i in range(len(column)):\n",
    "        if column[i] > 255/2:\n",
    "            notes.append(i+lowerBoundNote)\n",
    "    return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediary function\n",
    "def updateNotes(newNotes, prevNotes, resolution = 0.25): \n",
    "    res = {} \n",
    "    for note in newNotes:\n",
    "        if note in prevNotes:\n",
    "            res[note] = prevNotes[note] + resolution\n",
    "        else:\n",
    "            res[note] = resolution\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image2midi(image_path, lowerBoundNote = 21, resolution = 0.25):\n",
    "    \"\"\"\n",
    "    From an existing image:\n",
    "        - Convert to notes\n",
    "        - Save result as a midi file in the subfolder 'music_piece_name' of the 'data_output_sound' folder \n",
    "    \"\"\"\n",
    "    \n",
    "    output_folder = f\"../../data_output_midi/{image_path.split('/')[-2]}\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    output_filename = os.path.join(output_folder, image_path.split(\"/\")[-1].replace(\".png\",\".mid\"))\n",
    "    print(output_filename)\n",
    "    \n",
    "    with ImageOps.grayscale(Image.open(image_path)) as image:\n",
    "        im_arr = np.frombuffer(image.tobytes(), dtype=np.uint8)\n",
    "        print(im_arr.shape)\n",
    "        try:\n",
    "            im_arr = im_arr.reshape((image.size[1], image.size[0]))\n",
    "        except:\n",
    "            im_arr = im_arr.reshape((image.size[1], image.size[0],3))\n",
    "            im_arr = np.dot(im_arr, [0.33, 0.33, 0.33])\n",
    "    \n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    # create note and chord objects based on the values generated by the model\n",
    "\n",
    "    prev_notes = updateNotes(im_arr.T[0,:],{}, resolution = resolution)\n",
    "    for column in im_arr.T[1:,:]:\n",
    "        notes = column2notes(column, lowerBoundNote=lowerBoundNote)\n",
    "        # pattern is a chord\n",
    "        notes_in_chord = notes\n",
    "        old_notes = prev_notes.keys()\n",
    "        for old_note in old_notes:\n",
    "            if not old_note in notes_in_chord:\n",
    "                new_note = note.Note(old_note,quarterLength=prev_notes[old_note])\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                if offset - prev_notes[old_note] >= 0:\n",
    "                    new_note.offset = offset - prev_notes[old_note]\n",
    "                    output_notes.append(new_note)\n",
    "                elif offset == 0:\n",
    "                    new_note.offset = offset\n",
    "                    output_notes.append(new_note)                    \n",
    "                else:\n",
    "                    print(offset,prev_notes[old_note],old_note)\n",
    "\n",
    "        prev_notes = updateNotes(notes_in_chord,prev_notes)\n",
    "\n",
    "        # increase offset each iteration so that notes do not stack\n",
    "        offset += resolution\n",
    "\n",
    "    for old_note in prev_notes.keys():\n",
    "        new_note = note.Note(old_note,quarterLength=prev_notes[old_note])\n",
    "        new_note.storedInstrument = instrument.Piano()\n",
    "        new_note.offset = offset - prev_notes[old_note]\n",
    "\n",
    "        output_notes.append(new_note)\n",
    "\n",
    "    prev_notes = updateNotes(notes_in_chord,prev_notes)\n",
    "\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "    \n",
    "    midi_stream.write('midi', fp=output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data_output_midi/data_raw/\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../ChopAI/data_raw/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/agatheduranton/code/AgatheDuranton/ChopAI/ChopAI/ml_logic/full-data-preprocessing.ipynb Cell 15\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/agatheduranton/code/AgatheDuranton/ChopAI/ChopAI/ml_logic/full-data-preprocessing.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Testing the function\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/agatheduranton/code/AgatheDuranton/ChopAI/ChopAI/ml_logic/full-data-preprocessing.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m image_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m../../ChopAI/data_raw/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/agatheduranton/code/AgatheDuranton/ChopAI/ChopAI/ml_logic/full-data-preprocessing.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m image2midi(image_path)\n",
      "\u001b[1;32m/home/agatheduranton/code/AgatheDuranton/ChopAI/ChopAI/ml_logic/full-data-preprocessing.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/agatheduranton/code/AgatheDuranton/ChopAI/ChopAI/ml_logic/full-data-preprocessing.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m output_filename \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(output_folder, image_path\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m.png\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39m.mid\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/agatheduranton/code/AgatheDuranton/ChopAI/ChopAI/ml_logic/full-data-preprocessing.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(output_filename)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/agatheduranton/code/AgatheDuranton/ChopAI/ChopAI/ml_logic/full-data-preprocessing.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mwith\u001b[39;00m ImageOps\u001b[39m.\u001b[39mgrayscale(Image\u001b[39m.\u001b[39;49mopen(image_path)) \u001b[39mas\u001b[39;00m image:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/agatheduranton/code/AgatheDuranton/ChopAI/ChopAI/ml_logic/full-data-preprocessing.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     im_arr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mfrombuffer(image\u001b[39m.\u001b[39mtobytes(), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39muint8)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/agatheduranton/code/AgatheDuranton/ChopAI/ChopAI/ml_logic/full-data-preprocessing.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mprint\u001b[39m(im_arr\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/ChopAI/lib/python3.10/site-packages/PIL/Image.py:3218\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3215\u001b[0m     filename \u001b[39m=\u001b[39m fp\n\u001b[1;32m   3217\u001b[0m \u001b[39mif\u001b[39;00m filename:\n\u001b[0;32m-> 3218\u001b[0m     fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   3219\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   3221\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../ChopAI/data_raw/'"
     ]
    }
   ],
   "source": [
    "# Testing the function\n",
    "image_path = \"../../ChopAI/data_raw/\"\n",
    "image2midi(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From midi files, create a clean image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_midi_data_as_images(midi_path, output_folder_path, image_height = 106, image_length = 106):\n",
    "\n",
    "    \"\"\"\n",
    "    Iterate on all midi files from the 'midi_path' folder to:\n",
    "        - Keep music pieces with one piano only\n",
    "        - Transform the midi file into images\n",
    "        - Store all corresponding images into a 'music_piece' subfolder of the 'output_folder_path'\n",
    "    \"\"\"\n",
    "    # Storing all midi files into a 'files_raw' list\n",
    "    files_raw = [file for file in os.listdir(midi_path)]\n",
    "\n",
    "    # Storing all midi files with only one piano in a 'files' list\n",
    "    files = []\n",
    "    for file in files_raw:\n",
    "        try:\n",
    "            mid = converter.parse(f'{midi_path}/{file}')\n",
    "            file_instruments = instrument.partitionByInstrument(mid)\n",
    "            if len(file_instruments)==1:\n",
    "                files.append(file)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Iterating on all files from 'files' list to create images\n",
    "    for file in files:\n",
    "        file_path = f\"{midi_path}/{file}\"\n",
    "        midi2image(file_path, output_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the function\n",
    "midi_path ='../../data_test/Input_midi/'\n",
    "output_folder_path = '../../data_test/Input_image/'\n",
    "get_clean_midi_data_as_images(midi_path, output_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" midi_path ='../../data_raw/'\\noutput_folder_path = '../../data_image/'\\nget_clean_midi_data_as_images(midi_path, output_folder_path) \""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implementing the function for the entire dataset\n",
    "\"\"\" midi_path ='../../data_raw/'\n",
    "output_folder_path = '../../data_image/'\n",
    "get_clean_midi_data_as_images(midi_path, output_folder_path) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_images(input_path, output_path, height_image = 106, length_image = 106):\n",
    "    \"\"\"\n",
    "    Iterate on all images created in the 'input_path' folder:\n",
    "        - Resize images to height_image x length_image\n",
    "        - Transform them into pure black and white images\n",
    "        - Save them in a 'music piece' subfolder of the 'output_path' folder\n",
    "       \n",
    "    --> Input path: path to folder with input images (e.g., '../../data_test/Input_image')\n",
    "    --> Output path: path to folder where we wish to save output reshaped images (e.g., '../../data_test/Input_image_cleaned')\n",
    "    \"\"\"\n",
    "    \n",
    "    for music in os.listdir(input_path):\n",
    "        \n",
    "        output_folder = f'{output_path}/{music}' # Creating one sub_folder for each music piece in the 'output_path' folder\n",
    "        if not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "        \n",
    "        for image in os.listdir(f\"{input_path}/{music}\"):\n",
    "            image_path = f'{input_path}/{music}/{image}'\n",
    "            image_read = Image.open(image_path) # Reading each image\n",
    "            new_image = image_read.resize((106,106)) # Resizing each image\n",
    "            new_image = new_image.convert(\"1\") # Convert each image to pure black and white\n",
    "            new_image.save(f'{output_folder}/{image}') # Saving each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the function\n",
    "clean_images('../../data_test/Input_image', '../../data_test/Input_image_cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the function for the entire dataset\n",
    "input_path = '../../data_image'\n",
    "output_path = '../../data_image_cleaned'\n",
    "clean_images(input_path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images to be used are now stored in the 'data_image_cleaned' folder   \n",
    "Images are all of shape 106x106 and converted to pure black & white"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get clean array dataset from clean image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixels_array(input_path): \n",
    "    \"\"\"\n",
    "    Generate an array containing all images from 'input_path' folder in array format\n",
    "    \n",
    "    --> input_path = path of the folder containing clean images (e.g., '../../data_image_cleaned') \n",
    "    \"\"\"\n",
    "    \n",
    "    pixels = []\n",
    "    for music in os.listdir(input_path):\n",
    "        for image in os.listdir(f\"{input_path}/{music}\"):\n",
    "            image_path = f'{input_path}/{music}/{image}'\n",
    "            image_read = Image.open(image_path) # Reading each image\n",
    "            pixels_image = np.array(image_read.getdata()).astype('float32') # Store all pixel values in an array, each i_th-sequence contains the values of pixels in a i_th-row\n",
    "            pixels_image = pixels_image / 255.0 # All the values are 0 (black) and white (255). Normalize pixel values to be between 0 and 1\n",
    "            pixels.append(pixels_image.reshape(106, 106,1)) # Reshape pixels to be a matrix\n",
    "\n",
    "    pixels = np.array(pixels)\n",
    "    \n",
    "    return pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1507, 106, 106, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implementing the function for the entire dataset\n",
    "input_path = '../../data_image_cleaned'\n",
    "pixels = get_pixels_array(input_path)\n",
    "pixels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_from_pixels(pixels_matrix, image_number):\n",
    "    \"\"\"\n",
    "    Given a pixel matrix representing a dataset, get representation of one image (number image_number)\n",
    "    \"\"\"\n",
    "    plt.imshow(np.squeeze(pixels_matrix[image_number, :, :, :]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 417,
       "width": 425
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image_from_pixels(pixels, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset is now given in the form of a matrix containing 1507 images of shape 106 x 106"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input,Dense, Reshape, Flatten, BatchNormalization, Conv2D, Conv2DTranspose, LeakyReLU, Dropout\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_height = 106\n",
    "image_length = 106"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the standalone discriminator model\n",
    "def def_discriminator(in_shape=(image_height,image_length,1)): # Input is an image of shape 106 x 106 in black and white\n",
    "    \"\"\"\n",
    "    Returns a compiled discriminator model\n",
    "    \"\"\"\n",
    " \n",
    "    # Define inputs\n",
    "    inputs = Input(in_shape)\n",
    "    \n",
    "    # Block 1\n",
    "    convolutional_layer_1 = Conv2D(64, (3,3), strides=(2,2), padding='same', input_shape=in_shape) (inputs)\n",
    "    activation_1 = LeakyReLU(alpha=0.2) (convolutional_layer_1)\n",
    "    dropout_1 = Dropout(0.5) (activation_1)\n",
    "    \n",
    "    # Block 2\n",
    "    convolutional_layer_2 = Conv2D(64, (3,3), strides=(2,2), padding='same', input_shape=in_shape) (dropout_1)\n",
    "    activation_2 = LeakyReLU(alpha=0.2) (convolutional_layer_2)\n",
    "    dropout_2 = Dropout(0.5) (activation_2)\n",
    "    \n",
    "    # Classifier\n",
    "    flattened_layer = Flatten()(dropout_2)\n",
    "    batch_normalization_layer = BatchNormalization()(flattened_layer)\n",
    "    output_discriminator = Dense(1, activation=\"sigmoid\")(batch_normalization_layer)\n",
    "    \n",
    "    # Defining discrimnator model\n",
    "    model = Model(inputs, outputs = output_discriminator)\n",
    "    \n",
    "    # Compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                optimizer=opt,\n",
    "                metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 106, 106, 1)]     0         \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 53, 53, 64)        640       \n",
      "                                                                 \n",
      " leaky_re_lu_8 (LeakyReLU)   (None, 53, 53, 64)        0         \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 53, 53, 64)        0         \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 27, 27, 64)        36928     \n",
      "                                                                 \n",
      " leaky_re_lu_9 (LeakyReLU)   (None, 27, 27, 64)        0         \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 27, 27, 64)        0         \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 46656)             0         \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 46656)             186624    \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 46657     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 270849 (1.03 MB)\n",
      "Trainable params: 177537 (693.50 KB)\n",
      "Non-trainable params: 93312 (364.50 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator_model = def_discriminator()\n",
    "discriminator_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dimension = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the standalone generator model\n",
    "def def_generator(in_shape = latent_dimension):\n",
    "    \"\"\"\n",
    "    Returns a generator model WITHOUT compiling it\n",
    "    \"\"\"\n",
    "    # Defining inputs\n",
    "    inputs = Input(in_shape)\n",
    "    \n",
    "    # Block 1 - foundation for 53 x 53 images\n",
    "    n_nodes_1 = 128 * 53 * 53\n",
    "    dense_1 = Dense(n_nodes_1, input_dim=latent_dimension)(inputs)\n",
    "    activation_1 = LeakyReLU(alpha=0.2)(dense_1)\n",
    "    reshape_layer = Reshape( (53,53,128))(activation_1)\n",
    "    \n",
    "    # Block 2\n",
    "    dense_2 = Dense(1024)(reshape_layer)\n",
    "    conv2d_transposed_layer_1 = Conv2DTranspose(1024,(4,4), strides=(2,2), padding=\"same\")(dense_2)\n",
    "    \n",
    "    # Block 3\n",
    "    dense_3 = Dense(1024)(conv2d_transposed_layer_1)\n",
    "    activation_2 = LeakyReLU(alpha=0.2)(dense_3)\n",
    "    dense_4 = Dense(1024)(activation_2)\n",
    "    conv2d_transposed_layer_1 = Conv2DTranspose(1,(7,7), padding=\"same\", activation='sigmoid')(dense_4)\n",
    "    \n",
    "    # Generate model\n",
    "    model = Model(inputs, outputs=conv2d_transposed_layer_1)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 100)]             0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 359552)            36314752  \n",
      "                                                                 \n",
      " leaky_re_lu_10 (LeakyReLU)  (None, 359552)            0         \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 53, 53, 128)       0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 53, 53, 1024)      132096    \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTr  (None, 106, 106, 1024)    16778240  \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 106, 106, 1024)    1049600   \n",
      "                                                                 \n",
      " leaky_re_lu_11 (LeakyReLU)  (None, 106, 106, 1024)    0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 106, 106, 1024)    1049600   \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2D  (None, 106, 106, 1)       50177     \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 55374465 (211.24 MB)\n",
      "Trainable params: 55374465 (211.24 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator_model = def_generator()\n",
    "generator_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the combined generator and discriminator model, for updating the generator\n",
    "def def_gan(generator, discriminator):\n",
    "    \"\"\"\n",
    "    Returns a compiled GAN model\n",
    "    \"\"\"\n",
    "    # Make weights in the discriminator not trainable - train only generator weights\n",
    "    discriminator.trainable = False\n",
    "    \n",
    "    # Instantiate GAN model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add generator and discrimnator\n",
    "    model.add(generator)\n",
    "    model.add(discriminator)\n",
    "    \n",
    "    # Compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=opt)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " model_2 (Functional)        (None, 106, 106, 1)       55374465  \n",
      "                                                                 \n",
      " model_1 (Functional)        (None, 1)                 270849    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 55645314 (212.27 MB)\n",
      "Trainable params: 55374465 (211.24 MB)\n",
      "Non-trainable params: 270849 (1.03 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan_model = def_gan(generator_model, discriminator_model)\n",
    "gan_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will require one batch (or a half) batch of real images from the dataset for each update to the GAN model. A simple way to achieve this is to select a random sample of images from the dataset each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sample of 'real' images\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "    \"\"\"\n",
    "    Takes as input cleaned dataset and a number of samples to be generated\n",
    "    Returns a random sample of n_samples images and their corresponding label (=1 because these are real images)\n",
    "    \"\"\"\n",
    "    # Choose random instances (i.e., randomly select n_samples indexes from dataset)\n",
    "    iX = np.random.randint(0, pixels.shape[0], n_samples)\n",
    "    # Loading corresponding images\n",
    "    X = pixels[iX]\n",
    "    # Creating corresponding 'Real' (=1) labels\n",
    "    y = np.ones((n_samples, 1)) \n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs for the generator model are random points from latent space corresponding to a Gaussian distributed variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate points in latent space as input for the generator, following Gaussian distributed variable\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    # Generate (latent dimension x n_samples) array of random values taken from x axis of Normal Distribution\n",
    "    x_input = np.random.randn(latent_dim*n_samples)\n",
    "    # Reshape to have num_samples entries for each one with latent_dimension values\n",
    "    x_input = x_input.reshape(n_samples, latent_dimension)\n",
    "    return x_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "    # Generate points in latent space\n",
    "    x_input = generate_latent_points(latent_dim, n_samples)\n",
    "    # Predict outputs\n",
    "    X = generator.predict(x_input)\n",
    "    # Generate corresponding 'Fake' (=0) class labels\n",
    "    y = np.zeros((n_samples, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "n_epochs = 250\n",
    "n_batch = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function to display accuracy of the Discriminator in preidtcing correctly both real and fake music samples\n",
    "def show_current_discriminator_accuracy(discriminator_model, generator_model, pixels, latent_dimension = latent_dimension):\n",
    "    num_samples_to_test = 100\n",
    "    \n",
    "    #generate real music samples\n",
    "    X_real, y_real = generate_real_samples(pixels, num_samples_to_test)\n",
    "  \n",
    "    #generate fake music samples\n",
    "    X_fake, y_fake = generate_fake_samples(generator_model, latent_dimension, num_samples_to_test)\n",
    "    \n",
    "    #evaluate the accuracy of the discriminator on real music samples\n",
    "    _, accuracy_on_real = discriminator_model.evaluate(X_real, y_real, verbose=0)\n",
    "    #evaluate the accuracy of the discriminator on fake music samples\n",
    "    _, accuracy_on_fake = discriminator_model.evaluate(X_fake, y_fake, verbose=0)\n",
    "\n",
    "    #print results\n",
    "    print(\"   Current accuracy of the discriminator on real music samples:\", round(accuracy_on_real*100,3),\"%\")\n",
    "    print(\"   Current accuracy of the discriminator on fake music samples:\", round(accuracy_on_fake*100,3),\"% \\n\")\n",
    "\n",
    "    return accuracy_on_real, accuracy_on_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the GAN model\n",
    "def train(generator_model, discriminator_model, gan_model, dataset, latent_dim = latent_dimension, n_epochs=n_epochs, n_batch=n_batch):\n",
    "    bat_per_epo = int(dataset.shape[0] / n_batch) # Number of batches per epoch\n",
    "    half_batch = int(n_batch / 2) # Half the number of batches \n",
    "    \n",
    "    # Storing results along the epochs\n",
    "    # Dataframe containing loss and accuracy for each epoch\n",
    "    discriminator_info_per_epoch = pd.DataFrame(columns=['loss_discriminator_on_real_music', 'loss_generator_on_fake_music', 'accuracy_on_real', 'accuracy_on_fake'])\n",
    "    \n",
    "    accuracy_on_fake = 0\n",
    "    accuracy_on_real = 0\n",
    "    \n",
    "    # For each epoch\n",
    "    for i in range(n_epochs): # Enumerate epochs\n",
    "        \n",
    "        # For each batch\n",
    "        for j in range(bat_per_epo):\n",
    " \n",
    "            # get randomly selected 'real' samples\n",
    "            X_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "            # update discriminator model weights\n",
    "            d_loss1, _ = discriminator_model.train_on_batch(X_real, y_real)\n",
    "            \n",
    "            # generate 'fake' examples\n",
    "            X_fake, y_fake = generate_fake_samples(generator_model, latent_dim, half_batch)\n",
    "            # update discriminator model weights\n",
    "            d_loss2, _ = discriminator_model.train_on_batch(X_fake, y_fake)\n",
    "            \n",
    "            # prepare points in latent space as input for the generator\n",
    "            X_gan = generate_latent_points(latent_dim, n_batch)\n",
    "            # create inverted labels for the fake samples\n",
    "            y_gan = np.ones((n_batch, 1))\n",
    "            # update the generator via the discriminator's error\n",
    "            g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "\n",
    "        # print the current accuracy obtained in classifing correctly both real music samples and fake music samples (the ones generated by the generator)\n",
    "        accuracy_on_real, accuracy_on_fake = show_current_discriminator_accuracy(discriminator_model, generator_model, pixels, latent_dimension)\n",
    "        discriminator_info_per_epoch = discriminator_info_per_epoch.append({'loss_discriminator_on_real_music':  d_loss1, 'loss_generator_on_fake_music': g_loss, 'accuracy_on_real': accuracy_on_real, 'accuracy_on_fake': accuracy_on_fake}, ignore_index=True)\n",
    "        \n",
    "    return gan_model, discriminator_info_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "1/1 [==============================] - 8s 8s/step\n",
      "1/1 [==============================] - 9s 9s/step\n",
      "1/1 [==============================] - 10s 10s/step\n",
      "1/1 [==============================] - 9s 9s/step\n",
      "1/1 [==============================] - 10s 10s/step\n",
      "1/1 [==============================] - 8s 8s/step\n",
      "1/1 [==============================] - 8s 8s/step\n",
      "1/1 [==============================] - 8s 8s/step\n",
      "1/1 [==============================] - 9s 9s/step\n",
      "1/1 [==============================] - 9s 9s/step\n",
      "1/1 [==============================] - 9s 9s/step\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "1/1 [==============================] - 8s 8s/step\n",
      "1/1 [==============================] - 8s 8s/step\n",
      "1/1 [==============================] - 8s 8s/step\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "1/1 [==============================] - 10s 10s/step\n",
      "1/1 [==============================] - 8s 8s/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/agatheduranton/code/AgatheDuranton/ChopAI/ChopAI/ml_logic/full-data-preprocessing.ipynb Cell 52\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/agatheduranton/code/AgatheDuranton/ChopAI/ChopAI/ml_logic/full-data-preprocessing.ipynb#Y130sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m train(generator_model, discriminator_model, gan_model, pixels)\n",
      "\u001b[1;32m/home/agatheduranton/code/AgatheDuranton/ChopAI/ChopAI/ml_logic/full-data-preprocessing.ipynb Cell 52\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/agatheduranton/code/AgatheDuranton/ChopAI/ChopAI/ml_logic/full-data-preprocessing.ipynb#Y130sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m d_loss1, _ \u001b[39m=\u001b[39m discriminator_model\u001b[39m.\u001b[39mtrain_on_batch(X_real, y_real)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/agatheduranton/code/AgatheDuranton/ChopAI/ChopAI/ml_logic/full-data-preprocessing.ipynb#Y130sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# generate 'fake' examples\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/agatheduranton/code/AgatheDuranton/ChopAI/ChopAI/ml_logic/full-data-preprocessing.ipynb#Y130sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m X_fake, y_fake \u001b[39m=\u001b[39m generate_fake_samples(generator_model, latent_dim, half_batch)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/agatheduranton/code/AgatheDuranton/ChopAI/ChopAI/ml_logic/full-data-preprocessing.ipynb#Y130sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# update discriminator model weights\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/agatheduranton/code/AgatheDuranton/ChopAI/ChopAI/ml_logic/full-data-preprocessing.ipynb#Y130sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m d_loss2, _ \u001b[39m=\u001b[39m discriminator_model\u001b[39m.\u001b[39mtrain_on_batch(X_fake, y_fake)\n",
      "\u001b[1;32m/home/agatheduranton/code/AgatheDuranton/ChopAI/ChopAI/ml_logic/full-data-preprocessing.ipynb Cell 52\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/agatheduranton/code/AgatheDuranton/ChopAI/ChopAI/ml_logic/full-data-preprocessing.ipynb#Y130sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m x_input \u001b[39m=\u001b[39m generate_latent_points(latent_dim, n_samples)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/agatheduranton/code/AgatheDuranton/ChopAI/ChopAI/ml_logic/full-data-preprocessing.ipynb#Y130sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Predict outputs\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/agatheduranton/code/AgatheDuranton/ChopAI/ChopAI/ml_logic/full-data-preprocessing.ipynb#Y130sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m X \u001b[39m=\u001b[39m generator\u001b[39m.\u001b[39;49mpredict(x_input)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/agatheduranton/code/AgatheDuranton/ChopAI/ChopAI/ml_logic/full-data-preprocessing.ipynb#Y130sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Generate corresponding 'Fake' (=0) class labels\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/agatheduranton/code/AgatheDuranton/ChopAI/ChopAI/ml_logic/full-data-preprocessing.ipynb#Y130sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((n_samples, \u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/ChopAI/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/ChopAI/lib/python3.10/site-packages/keras/src/engine/training.py:2554\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2552\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n\u001b[1;32m   2553\u001b[0m     callbacks\u001b[39m.\u001b[39mon_predict_batch_begin(step)\n\u001b[0;32m-> 2554\u001b[0m     tmp_batch_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_function(iterator)\n\u001b[1;32m   2555\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   2556\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/ChopAI/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/ChopAI/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/ChopAI/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:864\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    862\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    863\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 864\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    865\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    866\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    867\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/ChopAI/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/ChopAI/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/ChopAI/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[1;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[1;32m    200\u001b[0m     )\n\u001b[1;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/ChopAI/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[1;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[1;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   1463\u001b[0m   )\n\u001b[1;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/ChopAI/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(generator_model, discriminator_model, gan_model, pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ChopAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
